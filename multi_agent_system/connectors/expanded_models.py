#!/usr/bin/env python3
"""
üåü BirLab AI - Expanded Models Collection
100+ AI Models Integration

This module adds 80+ additional AI models to reach approximately 100 total AIs.
Includes multiple providers, model variants, and specialized configurations.
"""

import os
from typing import Optional, Dict, Any, List
from .openai_connector import create_openai_agent
from .anthropic_connector import create_anthropic_agent
from .google_connector import create_google_agent, create_gemini_pro_agent, create_gemini_flash_agent
from .cohere_connector import create_cohere_agent
from .huggingface_connector import create_huggingface_agent
from .mistral_connector import create_mistral_agent
from .together_connector import create_together_agent
from .replicate_connector import create_replicate_agent
from .groq_connector import create_groq_agent
from .fireworks_connector import create_fireworks_agent
from .ollama_connector import create_ollama_agent


# üåü COMPREHENSIVE AI MODEL REGISTRY - 100+ MODELS
BIRLAB_AI_MODELS = {
    
    # ===== OPENAI MODELS (15 variants) =====
    "openai_gpt4_turbo": {
        "name": "üî• GPT-4 Turbo",
        "model": "gpt-4-turbo-preview",
        "provider": "openai",
        "capabilities": ["reasoning", "analysis", "code", "vision"],
        "context_length": 128000,
        "specialty": "Latest GPT-4 with extended context"
    },
    "openai_gpt4": {
        "name": "üß† GPT-4",
        "model": "gpt-4",
        "provider": "openai",
        "capabilities": ["advanced_reasoning", "complex_tasks"],
        "context_length": 8192
    },
    "openai_gpt4_32k": {
        "name": "üìö GPT-4 32K",
        "model": "gpt-4-32k",
        "provider": "openai",
        "capabilities": ["long_context", "document_analysis"],
        "context_length": 32768
    },
    "openai_gpt35_turbo": {
        "name": "‚ö° GPT-3.5 Turbo",
        "model": "gpt-3.5-turbo",
        "provider": "openai",
        "capabilities": ["fast_responses", "general_tasks"],
        "context_length": 4096
    },
    "openai_gpt35_turbo_16k": {
        "name": "üìñ GPT-3.5 Turbo 16K",
        "model": "gpt-3.5-turbo-16k",
        "provider": "openai",
        "capabilities": ["medium_context", "analysis"],
        "context_length": 16384
    },
    "openai_gpt35_instruct": {
        "name": "üìù GPT-3.5 Instruct",
        "model": "gpt-3.5-turbo-instruct",
        "provider": "openai",
        "capabilities": ["instruction_following", "completion"],
        "context_length": 4096
    },
    "openai_davinci_003": {
        "name": "üé® Davinci-003",
        "model": "text-davinci-003",
        "provider": "openai",
        "capabilities": ["creative_writing", "reasoning"],
        "context_length": 4097
    },
    "openai_curie": {
        "name": "‚ö° Curie",
        "model": "text-curie-001",
        "provider": "openai",
        "capabilities": ["fast_generation", "simple_tasks"],
        "context_length": 2049
    },
    "openai_babbage": {
        "name": "üîÑ Babbage",
        "model": "text-babbage-001",
        "provider": "openai",
        "capabilities": ["basic_tasks", "classification"],
        "context_length": 2049
    },
    "openai_ada": {
        "name": "üí´ Ada",
        "model": "text-ada-001",
        "provider": "openai",
        "capabilities": ["simple_tasks", "embeddings"],
        "context_length": 2049
    },
    "openai_codex": {
        "name": "üíª Codex",
        "model": "code-davinci-002",
        "provider": "openai",
        "capabilities": ["code_generation", "programming"],
        "context_length": 8001
    },
    "openai_embedding_large": {
        "name": "üîç Embedding Large",
        "model": "text-embedding-ada-002",
        "provider": "openai",
        "capabilities": ["embeddings", "similarity"],
        "context_length": 8191
    },
    "openai_whisper": {
        "name": "üéôÔ∏è Whisper",
        "model": "whisper-1",
        "provider": "openai",
        "capabilities": ["speech_to_text", "transcription"],
        "specialty": "Audio transcription"
    },
    "openai_dall_e_3": {
        "name": "üé® DALL-E 3",
        "model": "dall-e-3",
        "provider": "openai",
        "capabilities": ["image_generation", "creativity"],
        "specialty": "Advanced image generation"
    },
    "openai_dall_e_2": {
        "name": "üñºÔ∏è DALL-E 2",
        "model": "dall-e-2",
        "provider": "openai",
        "capabilities": ["image_generation", "art"],
        "specialty": "Image generation"
    },
    
    # ===== ANTHROPIC MODELS (8 variants) =====
    "anthropic_claude_3_opus": {
        "name": "üèõÔ∏è Claude-3 Opus",
        "model": "claude-3-opus-20240229",
        "provider": "anthropic",
        "capabilities": ["expert_reasoning", "research", "analysis"],
        "context_length": 200000,
        "specialty": "Highest capability Claude model"
    },
    "anthropic_claude_3_sonnet": {
        "name": "üéº Claude-3 Sonnet",
        "model": "claude-3-sonnet-20240229",
        "provider": "anthropic",
        "capabilities": ["balanced_performance", "reasoning"],
        "context_length": 200000
    },
    "anthropic_claude_3_haiku": {
        "name": "üå∏ Claude-3 Haiku",
        "model": "claude-3-haiku-20240307",
        "provider": "anthropic",
        "capabilities": ["fast_responses", "efficient"],
        "context_length": 200000
    },
    "anthropic_claude_2": {
        "name": "ü§ñ Claude-2",
        "model": "claude-2",
        "provider": "anthropic",
        "capabilities": ["general_tasks", "conversation"],
        "context_length": 100000
    },
    "anthropic_claude_2_1": {
        "name": "üîÑ Claude-2.1",
        "model": "claude-2.1",
        "provider": "anthropic",
        "capabilities": ["improved_reasoning", "reduced_hallucination"],
        "context_length": 200000
    },
    "anthropic_claude_instant": {
        "name": "‚ö° Claude Instant",
        "model": "claude-instant-1.2",
        "provider": "anthropic",
        "capabilities": ["fast_responses", "cost_effective"],
        "context_length": 100000
    },
    "anthropic_claude_instant_1": {
        "name": "üîÑ Claude Instant 1",
        "model": "claude-instant-1",
        "provider": "anthropic",
        "capabilities": ["quick_tasks", "efficient"],
        "context_length": 100000
    },
    "anthropic_claude_1": {
        "name": "ü•á Claude-1",
        "model": "claude-1",
        "provider": "anthropic",
        "capabilities": ["foundational_model", "conversation"],
        "context_length": 9000
    },
    
    # ===== GOOGLE AI MODELS (12 variants) =====
    "google_gemini_1_5_pro": {
        "name": "üß† Gemini 1.5 Pro",
        "model": "gemini-1.5-pro",
        "provider": "google",
        "capabilities": ["ultra_long_context", "multimodal", "reasoning"],
        "context_length": 2000000,
        "specialty": "2M token context window"
    },
    "google_gemini_1_5_flash": {
        "name": "‚ö° Gemini 1.5 Flash",
        "model": "gemini-1.5-flash",
        "provider": "google",
        "capabilities": ["fast_responses", "multimodal", "efficient"],
        "context_length": 1000000
    },
    "google_gemini_pro": {
        "name": "üåü Gemini Pro",
        "model": "gemini-pro",
        "provider": "google",
        "capabilities": ["reasoning", "analysis", "multimodal"],
        "context_length": 32768
    },
    "google_gemini_pro_vision": {
        "name": "üëÅÔ∏è Gemini Pro Vision",
        "model": "gemini-pro-vision",
        "provider": "google",
        "capabilities": ["vision", "multimodal", "image_analysis"],
        "context_length": 32768
    },
    "google_gemini_ultra": {
        "name": "üöÄ Gemini Ultra",
        "model": "gemini-ultra",
        "provider": "google",
        "capabilities": ["expert_tasks", "complex_reasoning", "multimodal"],
        "context_length": 32768,
        "specialty": "Most capable Gemini model"
    },
    "google_palm_2": {
        "name": "üå¥ PaLM 2",
        "model": "text-bison-001",
        "provider": "google",
        "capabilities": ["text_generation", "reasoning"],
        "context_length": 8192
    },
    "google_palm_2_chat": {
        "name": "üí¨ PaLM 2 Chat",
        "model": "chat-bison-001",
        "provider": "google",
        "capabilities": ["conversation", "dialogue"],
        "context_length": 8192
    },
    "google_palm_2_code": {
        "name": "üíª PaLM 2 Code",
        "model": "code-bison-001",
        "provider": "google",
        "capabilities": ["code_generation", "programming"],
        "context_length": 8192
    },
    "google_codey": {
        "name": "üîß Codey",
        "model": "codechat-bison-001",
        "provider": "google",
        "capabilities": ["code_chat", "programming_help"],
        "context_length": 8192
    },
    "google_embedding": {
        "name": "üîç PaLM Embedding",
        "model": "textembedding-gecko-001",
        "provider": "google",
        "capabilities": ["embeddings", "similarity"],
        "context_length": 3072
    },
    "google_imagen": {
        "name": "üé® Imagen",
        "model": "imagen-001",
        "provider": "google",
        "capabilities": ["image_generation", "text_to_image"],
        "specialty": "Google's image generation model"
    },
    "google_musiclm": {
        "name": "üéµ MusicLM",
        "model": "musiclm-001",
        "provider": "google",
        "capabilities": ["music_generation", "audio"],
        "specialty": "Music generation from text"
    },
    
    # ===== COHERE MODELS (6 variants) =====
    "cohere_command": {
        "name": "‚öîÔ∏è Command",
        "model": "command",
        "provider": "cohere",
        "capabilities": ["instruction_following", "reasoning"],
        "context_length": 4096
    },
    "cohere_command_light": {
        "name": "üí° Command Light",
        "model": "command-light",
        "provider": "cohere",
        "capabilities": ["fast_responses", "efficient"],
        "context_length": 4096
    },
    "cohere_command_nightly": {
        "name": "üåô Command Nightly",
        "model": "command-nightly",
        "provider": "cohere",
        "capabilities": ["latest_features", "experimental"],
        "context_length": 4096
    },
    "cohere_command_r": {
        "name": "üîÑ Command-R",
        "model": "command-r",
        "provider": "cohere",
        "capabilities": ["retrieval", "rag", "search"],
        "context_length": 128000
    },
    "cohere_command_r_plus": {
        "name": "‚ûï Command-R+",
        "model": "command-r-plus",
        "provider": "cohere",
        "capabilities": ["advanced_retrieval", "reasoning", "multilingual"],
        "context_length": 128000
    },
    "cohere_embed": {
        "name": "üîó Embed",
        "model": "embed-english-v3.0",
        "provider": "cohere",
        "capabilities": ["embeddings", "similarity", "semantic_search"],
        "context_length": 512
    },
    
    # ===== MISTRAL AI MODELS (8 variants) =====
    "mistral_large": {
        "name": "üè∞ Mistral Large",
        "model": "mistral-large-latest",
        "provider": "mistral",
        "capabilities": ["reasoning", "multilingual", "code"],
        "context_length": 32768
    },
    "mistral_medium": {
        "name": "üèõÔ∏è Mistral Medium",
        "model": "mistral-medium-latest",
        "provider": "mistral",
        "capabilities": ["balanced_performance", "general_tasks"],
        "context_length": 32768
    },
    "mistral_small": {
        "name": "üè† Mistral Small",
        "model": "mistral-small-latest",
        "provider": "mistral",
        "capabilities": ["efficient", "cost_effective"],
        "context_length": 32768
    },
    "mistral_tiny": {
        "name": "üèòÔ∏è Mistral Tiny",
        "model": "mistral-tiny",
        "provider": "mistral",
        "capabilities": ["lightweight", "fast"],
        "context_length": 32768
    },
    "mistral_7b": {
        "name": "üî¢ Mistral 7B",
        "model": "open-mistral-7b",
        "provider": "mistral",
        "capabilities": ["open_source", "efficient"],
        "context_length": 32768
    },
    "mixtral_8x7b": {
        "name": "üîÄ Mixtral 8x7B",
        "model": "open-mixtral-8x7b",
        "provider": "mistral",
        "capabilities": ["mixture_of_experts", "multilingual"],
        "context_length": 32768
    },
    "mixtral_8x22b": {
        "name": "üöÄ Mixtral 8x22B",
        "model": "open-mixtral-8x22b",
        "provider": "mistral",
        "capabilities": ["large_moe", "advanced_reasoning"],
        "context_length": 65536
    },
    "mistral_embed": {
        "name": "üîç Mistral Embed",
        "model": "mistral-embed",
        "provider": "mistral",
        "capabilities": ["embeddings", "retrieval"],
        "context_length": 8192
    },
    
    # ===== HUGGING FACE MODELS (15 variants) =====
    "hf_llama_2_70b": {
        "name": "ü¶ô Llama 2 70B",
        "model": "meta-llama/Llama-2-70b-chat-hf",
        "provider": "huggingface",
        "capabilities": ["large_model", "reasoning", "chat"],
        "context_length": 4096
    },
    "hf_llama_2_13b": {
        "name": "ü¶ô Llama 2 13B",
        "model": "meta-llama/Llama-2-13b-chat-hf",
        "provider": "huggingface",
        "capabilities": ["medium_model", "chat"],
        "context_length": 4096
    },
    "hf_llama_2_7b": {
        "name": "ü¶ô Llama 2 7B",
        "model": "meta-llama/Llama-2-7b-chat-hf",
        "provider": "huggingface",
        "capabilities": ["efficient", "chat"],
        "context_length": 4096
    },
    "hf_code_llama_34b": {
        "name": "üíª Code Llama 34B",
        "model": "codellama/CodeLlama-34b-Instruct-hf",
        "provider": "huggingface",
        "capabilities": ["code_generation", "programming"],
        "context_length": 16384
    },
    "hf_falcon_180b": {
        "name": "ü¶Ö Falcon 180B",
        "model": "tiiuae/falcon-180b-chat",
        "provider": "huggingface",
        "capabilities": ["large_model", "multilingual"],
        "context_length": 2048
    },
    "hf_falcon_40b": {
        "name": "ü¶Ö Falcon 40B",
        "model": "tiiuae/falcon-40b-instruct",
        "provider": "huggingface",
        "capabilities": ["medium_model", "instruction_following"],
        "context_length": 2048
    },
    "hf_falcon_7b": {
        "name": "ü¶Ö Falcon 7B",
        "model": "tiiuae/falcon-7b-instruct",
        "provider": "huggingface",
        "capabilities": ["efficient", "instruction_following"],
        "context_length": 2048
    },
    "hf_vicuna_33b": {
        "name": "ü¶ô Vicuna 33B",
        "model": "lmsys/vicuna-33b-v1.3",
        "provider": "huggingface",
        "capabilities": ["conversation", "helpful"],
        "context_length": 2048
    },
    "hf_vicuna_13b": {
        "name": "ü¶ô Vicuna 13B",
        "model": "lmsys/vicuna-13b-v1.5",
        "provider": "huggingface",
        "capabilities": ["conversation", "efficient"],
        "context_length": 4096
    },
    "hf_alpaca_7b": {
        "name": "ü¶ô Alpaca 7B",
        "model": "chavinlo/alpaca-native",
        "provider": "huggingface",
        "capabilities": ["instruction_tuned", "helpful"],
        "context_length": 2048
    },
    "hf_flan_t5_xxl": {
        "name": "üìö FLAN-T5 XXL",
        "model": "google/flan-t5-xxl",
        "provider": "huggingface",
        "capabilities": ["instruction_following", "reasoning"],
        "context_length": 512
    },
    "hf_flan_ul2": {
        "name": "üìñ FLAN-UL2",
        "model": "google/flan-ul2",
        "provider": "huggingface",
        "capabilities": ["unified_language", "versatile"],
        "context_length": 2048
    },
    "hf_bloom_176b": {
        "name": "üå∏ BLOOM 176B",
        "model": "bigscience/bloom",
        "provider": "huggingface",
        "capabilities": ["multilingual", "large_scale"],
        "context_length": 2048
    },
    "hf_gpt_j_6b": {
        "name": "ü§ñ GPT-J 6B",
        "model": "EleutherAI/gpt-j-6b",
        "provider": "huggingface",
        "capabilities": ["open_source", "general"],
        "context_length": 2048
    },
    "hf_gpt_neox_20b": {
        "name": "üî• GPT-NeoX 20B",
        "model": "EleutherAI/gpt-neox-20b",
        "provider": "huggingface",
        "capabilities": ["large_context", "reasoning"],
        "context_length": 2048
    },
    
    # ===== TOGETHER AI MODELS (10 variants) =====
    "together_llama_2_70b": {
        "name": "ü¶ô Llama 2 70B (Together)",
        "model": "meta-llama/Llama-2-70b-chat-hf",
        "provider": "together",
        "capabilities": ["large_model", "optimized_inference"],
        "context_length": 4096
    },
    "together_code_llama_34b": {
        "name": "üíª Code Llama 34B (Together)",
        "model": "codellama/CodeLlama-34b-Instruct-hf",
        "provider": "together",
        "capabilities": ["code_generation", "fast_inference"],
        "context_length": 16384
    },
    "together_falcon_40b": {
        "name": "ü¶Ö Falcon 40B (Together)",
        "model": "togethercomputer/falcon-40b-instruct",
        "provider": "together",
        "capabilities": ["instruction_following", "optimized"],
        "context_length": 2048
    },
    "together_mpt_30b": {
        "name": "üîß MPT 30B",
        "model": "mosaicml/mpt-30b-chat",
        "provider": "together",
        "capabilities": ["chat", "commercial_use"],
        "context_length": 8192
    },
    "together_redpajama_7b": {
        "name": "üî¥ RedPajama 7B",
        "model": "togethercomputer/RedPajama-INCITE-7B-Chat",
        "provider": "together",
        "capabilities": ["efficient", "open_source"],
        "context_length": 2048
    },
    "together_alpaca_7b": {
        "name": "ü¶ô Alpaca 7B (Together)",
        "model": "togethercomputer/alpaca-7b",
        "provider": "together",
        "capabilities": ["instruction_tuned", "fast"],
        "context_length": 2048
    },
    "together_vicuna_13b": {
        "name": "ü¶ô Vicuna 13B (Together)",
        "model": "lmsys/vicuna-13b-v1.5",
        "provider": "together",
        "capabilities": ["conversation", "helpful"],
        "context_length": 4096
    },
    "together_wizardlm_13b": {
        "name": "üßô WizardLM 13B",
        "model": "WizardLM/WizardLM-13B-V1.2",
        "provider": "together",
        "capabilities": ["instruction_following", "reasoning"],
        "context_length": 2048
    },
    "together_openchat_13b": {
        "name": "üí¨ OpenChat 13B",
        "model": "openchat/openchat",
        "provider": "together",
        "capabilities": ["conversation", "helpful"],
        "context_length": 2048
    },
    "together_nous_hermes_13b": {
        "name": "‚ö° Nous Hermes 13B",
        "model": "NousResearch/Nous-Hermes-13b",
        "provider": "together",
        "capabilities": ["reasoning", "creative"],
        "context_length": 2048
    },
    
    # ===== OLLAMA LOCAL MODELS (20 variants) =====
    "ollama_llama3_8b": {
        "name": "ü¶ô Llama 3 8B (Local)",
        "model": "llama3:8b",
        "provider": "ollama",
        "capabilities": ["local_inference", "privacy", "fast"],
        "context_length": 8192,
        "specialty": "Latest Meta model, runs locally"
    },
    "ollama_llama3_70b": {
        "name": "ü¶ô Llama 3 70B (Local)",
        "model": "llama3:70b",
        "provider": "ollama",
        "capabilities": ["local_inference", "large_model", "privacy"],
        "context_length": 8192
    },
    "ollama_llama2_7b": {
        "name": "ü¶ô Llama 2 7B (Local)",
        "model": "llama2:7b",
        "provider": "ollama",
        "capabilities": ["local_inference", "efficient"],
        "context_length": 4096
    },
    "ollama_llama2_13b": {
        "name": "ü¶ô Llama 2 13B (Local)",
        "model": "llama2:13b",
        "provider": "ollama",
        "capabilities": ["local_inference", "balanced"],
        "context_length": 4096
    },
    "ollama_llama2_70b": {
        "name": "ü¶ô Llama 2 70B (Local)",
        "model": "llama2:70b",
        "provider": "ollama",
        "capabilities": ["local_inference", "large_model"],
        "context_length": 4096
    },
    "ollama_codellama_7b": {
        "name": "üíª Code Llama 7B (Local)",
        "model": "codellama:7b",
        "provider": "ollama",
        "capabilities": ["code_generation", "local_inference"],
        "context_length": 16384
    },
    "ollama_codellama_13b": {
        "name": "üíª Code Llama 13B (Local)",
        "model": "codellama:13b",
        "provider": "ollama",
        "capabilities": ["code_generation", "local_inference"],
        "context_length": 16384
    },
    "ollama_codellama_34b": {
        "name": "üíª Code Llama 34B (Local)",
        "model": "codellama:34b",
        "provider": "ollama",
        "capabilities": ["advanced_coding", "local_inference"],
        "context_length": 16384
    },
    "ollama_mistral_7b": {
        "name": "üå™Ô∏è Mistral 7B (Local)",
        "model": "mistral:7b",
        "provider": "ollama",
        "capabilities": ["multilingual", "local_inference"],
        "context_length": 32768
    },
    "ollama_mixtral_8x7b": {
        "name": "üîÄ Mixtral 8x7B (Local)",
        "model": "mixtral:8x7b",
        "provider": "ollama",
        "capabilities": ["mixture_of_experts", "local_inference"],
        "context_length": 32768
    },
    "ollama_neural_chat_7b": {
        "name": "üß† Neural Chat 7B (Local)",
        "model": "neural-chat:7b",
        "provider": "ollama",
        "capabilities": ["conversation", "local_inference"],
        "context_length": 4096
    },
    "ollama_starling_7b": {
        "name": "üê¶ Starling 7B (Local)",
        "model": "starling-lm:7b",
        "provider": "ollama",
        "capabilities": ["helpful", "local_inference"],
        "context_length": 8192
    },
    "ollama_orca_mini_3b": {
        "name": "üêã Orca Mini 3B (Local)",
        "model": "orca-mini:3b",
        "provider": "ollama",
        "capabilities": ["lightweight", "local_inference"],
        "context_length": 2048
    },
    "ollama_vicuna_7b": {
        "name": "ü¶ô Vicuna 7B (Local)",
        "model": "vicuna:7b",
        "provider": "ollama",
        "capabilities": ["conversation", "local_inference"],
        "context_length": 2048
    },
    "ollama_wizard_vicuna_13b": {
        "name": "üßô Wizard Vicuna 13B (Local)",
        "model": "wizard-vicuna:13b",
        "provider": "ollama",
        "capabilities": ["instruction_following", "local_inference"],
        "context_length": 2048
    },
    "ollama_dolphin_phi_2_7b": {
        "name": "üê¨ Dolphin Phi 2.7B (Local)",
        "model": "dolphin-phi:2.7b",
        "provider": "ollama",
        "capabilities": ["efficient", "uncensored", "local_inference"],
        "context_length": 2048
    },
    "ollama_phind_codellama_34b": {
        "name": "üîç Phind CodeLlama 34B (Local)",
        "model": "phind-codellama:34b",
        "provider": "ollama",
        "capabilities": ["code_search", "programming", "local_inference"],
        "context_length": 16384
    },
    "ollama_deepseek_coder_6_7b": {
        "name": "ü§ø DeepSeek Coder 6.7B (Local)",
        "model": "deepseek-coder:6.7b",
        "provider": "ollama",
        "capabilities": ["code_generation", "local_inference"],
        "context_length": 16384
    },
    "ollama_magicoder_7b": {
        "name": "üé© MagiCoder 7B (Local)",
        "model": "magicoder:7b",
        "provider": "ollama",
        "capabilities": ["code_generation", "instruction_following", "local_inference"],
        "context_length": 16384
    },
    "ollama_stable_code_3b": {
        "name": "‚öñÔ∏è Stable Code 3B (Local)",
        "model": "stable-code:3b",
        "provider": "ollama",
        "capabilities": ["code_completion", "lightweight", "local_inference"],
        "context_length": 16384
    },
}


def get_all_birlab_ai_models() -> Dict[str, Dict[str, Any]]:
    """Get the complete registry of 100+ BirLab AI models"""
    return BIRLAB_AI_MODELS


def create_birlab_ai_agent(model_id: str, api_key: Optional[str] = None, **config):
    """
    Create any BirLab AI agent by model ID
    
    Args:
        model_id: ID from BIRLAB_AI_MODELS registry
        api_key: API key for the provider
        **config: Additional configuration
    
    Returns:
        Configured AI agent
    """
    if model_id not in BIRLAB_AI_MODELS:
        raise ValueError(f"Model '{model_id}' not found in BirLab AI registry. Available: {list(BIRLAB_AI_MODELS.keys())}")
    
    model_config = BIRLAB_AI_MODELS[model_id]
    provider = model_config["provider"]
    model_name = model_config["model"]
    capabilities = model_config.get("capabilities", [])
    
    # Merge configuration
    agent_config = {
        "capabilities": capabilities,
        **config
    }
    
    # Create agent based on provider
    if provider == "openai":
        return create_openai_agent(model_name, api_key, **agent_config)
    elif provider == "anthropic":
        return create_anthropic_agent(model_name, api_key, **agent_config)
    elif provider == "google":
        return create_google_agent(model_name, api_key, **agent_config)
    elif provider == "cohere":
        return create_cohere_agent(model_name, api_key, **agent_config)
    elif provider == "huggingface":
        return create_huggingface_agent(model_name, api_key, **agent_config)
    elif provider == "mistral":
        return create_mistral_agent(model_name, api_key, **agent_config)
    elif provider == "together":
        return create_together_agent(model_name, api_key, **agent_config)
    elif provider == "ollama":
        return create_ollama_agent(model_name, **agent_config)
    else:
        raise ValueError(f"Provider '{provider}' not supported")


def get_models_by_provider(provider: str) -> List[str]:
    """Get all model IDs for a specific provider"""
    return [
        model_id for model_id, config in BIRLAB_AI_MODELS.items()
        if config["provider"] == provider
    ]


def get_models_by_capability(capability: str) -> List[str]:
    """Get all model IDs that have a specific capability"""
    return [
        model_id for model_id, config in BIRLAB_AI_MODELS.items()
        if capability in config.get("capabilities", [])
    ]


def get_provider_count() -> Dict[str, int]:
    """Get count of models per provider"""
    counts = {}
    for config in BIRLAB_AI_MODELS.values():
        provider = config["provider"]
        counts[provider] = counts.get(provider, 0) + 1
    return counts


def print_birlab_ai_registry():
    """Print a summary of all available BirLab AI models"""
    provider_counts = get_provider_count()
    total_models = len(BIRLAB_AI_MODELS)
    
    print(f"üåü BirLab AI Model Registry - {total_models} AI Models Available! üåü")
    print("=" * 60)
    
    for provider, count in sorted(provider_counts.items()):
        print(f"üìä {provider.title()}: {count} models")
    
    print(f"\nüéØ Total Models: {total_models}")
    print(f"üè¢ Providers: {len(provider_counts)}")
    
    # Show some examples
    print(f"\nüî• Featured Models:")
    featured = [
        "google_gemini_1_5_pro",
        "openai_gpt4_turbo", 
        "anthropic_claude_3_opus",
        "ollama_llama3_70b",
        "mistral_large"
    ]
    
    for model_id in featured:
        if model_id in BIRLAB_AI_MODELS:
            model = BIRLAB_AI_MODELS[model_id]
            print(f"  ‚Ä¢ {model['name']} - {model.get('specialty', 'Advanced AI model')}")


# Convenience functions for specialized model types
def create_coding_agents(api_keys: Dict[str, str]) -> List:
    """Create a collection of coding-specialized agents"""
    coding_models = get_models_by_capability("code_generation")
    agents = []
    
    for model_id in coding_models[:10]:  # Top 10 coding models
        try:
            agent = create_birlab_ai_agent(model_id, api_keys.get(BIRLAB_AI_MODELS[model_id]["provider"]))
            agents.append((model_id, agent))
        except Exception as e:
            print(f"‚ö†Ô∏è Could not create {model_id}: {e}")
    
    return agents


def create_local_agents() -> List:
    """Create a collection of local/privacy-focused agents"""
    local_models = get_models_by_provider("ollama")
    agents = []
    
    for model_id in local_models:
        try:
            agent = create_birlab_ai_agent(model_id)
            agents.append((model_id, agent))
        except Exception as e:
            print(f"‚ö†Ô∏è Could not create local model {model_id}: {e}")
    
    return agents


def create_multimodal_agents(api_keys: Dict[str, str]) -> List:
    """Create a collection of multimodal agents"""
    multimodal_models = get_models_by_capability("multimodal")
    agents = []
    
    for model_id in multimodal_models:
        try:
            provider = BIRLAB_AI_MODELS[model_id]["provider"]
            agent = create_birlab_ai_agent(model_id, api_keys.get(provider))
            agents.append((model_id, agent))
        except Exception as e:
            print(f"‚ö†Ô∏è Could not create multimodal model {model_id}: {e}")
    
    return agents


if __name__ == "__main__":
    print_birlab_ai_registry() 