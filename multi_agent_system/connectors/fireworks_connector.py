import asyncio
import json
from typing import Dict, Any, Optional
import aiohttp
from .base_connector import AIModelConnector


class FireworksConnector(AIModelConnector):
    """
    Connector for Fireworks AI - fast inference for open source models.
    
    Fireworks provides high-speed inference for a wide variety of 
    open-source models with production-ready scaling.
    """
    
    def __init__(self, model_name: str = "accounts/fireworks/models/llama-v3p1-70b-instruct", 
                 api_key: Optional[str] = None,
                 base_url: str = "https://api.fireworks.ai/inference/v1", **config):
        super().__init__(model_name, config)
        self.api_key = api_key or config.get("api_key")
        self.base_url = base_url.rstrip('/')
        
        if not self.api_key:
            raise ValueError("Fireworks API key is required")
    
    async def generate_response(self, prompt: str, **kwargs) -> str:
        """
        Generate a response using Fireworks' fast inference API.
        
        Args:
            prompt: The input prompt
            **kwargs: Additional parameters
            
        Returns:
            Generated response string
        """
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        # Use chat format for most models
        messages = [{"role": "user", "content": prompt}]
        
        # Add system message if provided
        if "system_prompt" in kwargs:
            messages.insert(0, {"role": "system", "content": kwargs["system_prompt"]})
        
        payload = {
            "model": self.model_name,
            "messages": messages,
            "temperature": kwargs.get("temperature", 0.7),
            "max_tokens": kwargs.get("max_tokens", 1000),
            "top_p": kwargs.get("top_p", 1.0),
            "top_k": kwargs.get("top_k", 40),
            "frequency_penalty": kwargs.get("frequency_penalty", 0),
            "presence_penalty": kwargs.get("presence_penalty", 0),
            "n": 1,
            "stream": False
        }
        
        # Add stop sequences if provided
        if "stop" in kwargs:
            payload["stop"] = kwargs["stop"] if isinstance(kwargs["stop"], list) else [kwargs["stop"]]
        
        async with aiohttp.ClientSession() as session:
            try:
                async with session.post(
                    f"{self.base_url}/chat/completions",
                    headers=headers,
                    json=payload,
                    timeout=aiohttp.ClientTimeout(total=60)
                ) as response:
                    
                    if response.status != 200:
                        error_text = await response.text()
                        raise Exception(f"Fireworks API error {response.status}: {error_text}")
                    
                    result = await response.json()
                    
                    if "choices" not in result or not result["choices"]:
                        raise Exception("No response generated by Fireworks")
                    
                    return result["choices"][0]["message"]["content"]
                    
            except aiohttp.ClientError as e:
                raise Exception(f"Network error communicating with Fireworks: {str(e)}")
            except json.JSONDecodeError as e:
                raise Exception(f"Failed to parse Fireworks response: {str(e)}")
    
    async def stream_response(self, prompt: str, **kwargs):
        """
        Generate a streaming response from Fireworks.
        
        Args:
            prompt: The input prompt
            **kwargs: Additional parameters
            
        Yields:
            Response chunks as they arrive
        """
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        messages = [{"role": "user", "content": prompt}]
        
        if "system_prompt" in kwargs:
            messages.insert(0, {"role": "system", "content": kwargs["system_prompt"]})
        
        payload = {
            "model": self.model_name,
            "messages": messages,
            "temperature": kwargs.get("temperature", 0.7),
            "max_tokens": kwargs.get("max_tokens", 1000),
            "top_p": kwargs.get("top_p", 1.0),
            "stream": True
        }
        
        async with aiohttp.ClientSession() as session:
            try:
                async with session.post(
                    f"{self.base_url}/chat/completions",
                    headers=headers,
                    json=payload,
                    timeout=aiohttp.ClientTimeout(total=120)
                ) as response:
                    
                    if response.status != 200:
                        error_text = await response.text()
                        raise Exception(f"Fireworks streaming error {response.status}: {error_text}")
                    
                    async for line in response.content:
                        line = line.decode('utf-8').strip()
                        if line.startswith('data: '):
                            data_str = line[6:]
                            if data_str == '[DONE]':
                                break
                            try:
                                data = json.loads(data_str)
                                if "choices" in data and data["choices"]:
                                    delta = data["choices"][0].get("delta", {})
                                    if "content" in delta:
                                        yield delta["content"]
                            except json.JSONDecodeError:
                                continue
                    
            except aiohttp.ClientError as e:
                raise Exception(f"Network error during streaming: {str(e)}")
    
    async def validate_connection(self) -> bool:
        """Validate connection to Fireworks API"""
        try:
            response = await self.generate_response(
                "Hello",
                max_tokens=5,
                temperature=0
            )
            return len(response.strip()) > 0
        except Exception as e:
            self.logger.error(f"Connection validation failed: {e}")
            return False
    
    def get_model_info(self) -> Dict[str, Any]:
        """Get information about the Fireworks model"""
        model_info = {
            "provider": "Fireworks AI",
            "model_name": self.model_name,
            "base_url": self.base_url,
            "infrastructure": "Optimized for fast open-source model inference"
        }
        
        # Extract model family from full model path
        model_simple = self.model_name.split("/")[-1] if "/" in self.model_name else self.model_name
        
        # Add model-specific information
        if "llama" in model_simple.lower():
            model_info.update({
                "model_family": "Llama",
                "capabilities": [
                    "text_generation",
                    "conversation",
                    "reasoning",
                    "fast_inference",
                    "code_generation"
                ]
            })
            
            if "70b" in model_simple.lower():
                model_info.update({
                    "size": "70B parameters",
                    "performance": "High quality, fast inference"
                })
            elif "8b" in model_simple.lower():
                model_info.update({
                    "size": "8B parameters",
                    "performance": "Very fast, good quality"
                })
            elif "405b" in model_simple.lower():
                model_info.update({
                    "size": "405B parameters",
                    "performance": "Highest quality available"
                })
                
        elif "mixtral" in model_simple.lower():
            model_info.update({
                "model_family": "Mixtral",
                "architecture": "Mixture of Experts",
                "capabilities": [
                    "text_generation",
                    "multilingual",
                    "reasoning",
                    "fast_inference"
                ],
                "size": "8x7B or 8x22B parameters",
                "performance": "Fast MoE inference"
            })
            
        elif "qwen" in model_simple.lower():
            model_info.update({
                "model_family": "Qwen",
                "capabilities": [
                    "text_generation",
                    "multilingual",
                    "reasoning",
                    "math",
                    "coding"
                ],
                "origin": "Alibaba Cloud"
            })
            
        elif "deepseek" in model_simple.lower():
            model_info.update({
                "model_family": "DeepSeek",
                "capabilities": [
                    "code_generation",
                    "reasoning",
                    "math",
                    "text_generation"
                ],
                "specialization": "Code and reasoning"
            })
            
        elif "yi" in model_simple.lower():
            model_info.update({
                "model_family": "Yi",
                "capabilities": [
                    "text_generation",
                    "long_context",
                    "multilingual",
                    "reasoning"
                ],
                "origin": "01.AI"
            })
        
        # Add Fireworks-specific features
        model_info["fireworks_features"] = [
            "Production-ready scaling",
            "Fast cold starts",
            "Auto-scaling",
            "Cost-optimized inference"
        ]
        
        return model_info
    
    async def get_available_models(self) -> Dict[str, Any]:
        """Get list of available models from Fireworks"""
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(
                    f"{self.base_url}/models",
                    headers=headers,
                    timeout=aiohttp.ClientTimeout(total=30)
                ) as response:
                    
                    if response.status != 200:
                        return {"error": f"API error {response.status}"}
                    
                    result = await response.json()
                    
                    models = []
                    for model in result.get("data", []):
                        models.append({
                            "id": model.get("id"),
                            "object": model.get("object"),
                            "created": model.get("created"),
                            "owned_by": model.get("owned_by", "Fireworks"),
                            "context_length": model.get("context_length", "Unknown")
                        })
                    
                    return {
                        "available_models": models,
                        "total_count": len(models),
                        "note": "Open source models optimized for fast inference"
                    }
                    
        except Exception as e:
            self.logger.error(f"Failed to get available models: {e}")
            return {"error": str(e)}


# Convenience functions
def create_fireworks_agent(model_name: str = "accounts/fireworks/models/llama-v3p1-70b-instruct",
                          api_key: Optional[str] = None,
                          capabilities: Optional[list] = None,
                          system_prompt: str = "",
                          **config) -> 'AIAgent':
    """Create an AI agent using Fireworks' fast inference"""
    from .base_connector import AIAgent
    
    # Extract simple model name for capability detection
    model_simple = model_name.split("/")[-1] if "/" in model_name else model_name
    
    if capabilities is None:
        if "llama" in model_simple.lower():
            capabilities = [
                "fast_inference",
                "conversation",
                "reasoning",
                "text_generation",
                "code_generation"
            ]
        elif "mixtral" in model_simple.lower():
            capabilities = [
                "fast_inference",
                "multilingual",
                "reasoning",
                "mixture_of_experts"
            ]
        elif "qwen" in model_simple.lower():
            capabilities = [
                "fast_inference",
                "multilingual",
                "math",
                "coding",
                "reasoning"
            ]
        elif "deepseek" in model_simple.lower():
            capabilities = [
                "fast_inference",
                "code_generation",
                "math",
                "reasoning"
            ]
        else:
            capabilities = [
                "fast_inference",
                "text_generation",
                "conversation"
            ]
    
    connector = FireworksConnector(model_name, api_key, **config)
    agent = AIAgent(connector, capabilities, name=f"Fireworks_{model_simple.split('-')[0]}")
    
    if not system_prompt:
        system_prompt = f"""You are powered by Fireworks AI running {model_simple}. 
        You provide fast, efficient responses using optimized open-source models. 
        Combine speed with quality to deliver excellent results."""
    
    agent.set_system_prompt(system_prompt)
    return agent


def create_fireworks_llama_agent(size: str = "70b", api_key: Optional[str] = None, **config):
    """Create a Llama agent via Fireworks with specified size"""
    size_mapping = {
        "8b": "accounts/fireworks/models/llama-v3p1-8b-instruct",
        "70b": "accounts/fireworks/models/llama-v3p1-70b-instruct", 
        "405b": "accounts/fireworks/models/llama-v3p1-405b-instruct"
    }
    
    model_name = size_mapping.get(size, size_mapping["70b"])
    
    return create_fireworks_agent(
        model_name=model_name,
        api_key=api_key,
        capabilities=[
            "fast_llama_inference",
            "conversation",
            "reasoning",
            "code_generation",
            "analysis"
        ],
        system_prompt=f"""You are Llama 3.1 {size.upper()} running on Fireworks AI's optimized infrastructure. 
        You provide fast, high-quality responses with the power of Meta's latest model.""",
        **config
    )


def create_fireworks_mixtral_agent(api_key: Optional[str] = None, **config):
    """Create a Mixtral MoE agent via Fireworks"""
    return create_fireworks_agent(
        model_name="accounts/fireworks/models/mixtral-8x7b-instruct",
        api_key=api_key,
        capabilities=[
            "fast_moe_inference",
            "multilingual",
            "reasoning",
            "efficient_scaling",
            "mixture_of_experts"
        ],
        system_prompt="""You are Mixtral 8x7B running on Fireworks AI with mixture-of-experts architecture. 
        You excel at multilingual tasks and efficient reasoning while maintaining fast inference speeds.""",
        **config
    )


def create_fireworks_code_agent(api_key: Optional[str] = None, **config):
    """Create a code-specialized agent via Fireworks"""
    return create_fireworks_agent(
        model_name="accounts/fireworks/models/deepseek-coder-v2-lite-instruct",
        api_key=api_key,
        capabilities=[
            "fast_code_generation",
            "code_completion",
            "debugging",
            "code_explanation",
            "programming_languages"
        ],
        system_prompt="""You are DeepSeek Coder running on Fireworks AI, specialized in programming and code generation. 
        You provide fast, accurate code solutions across multiple programming languages.""",
        **config
    )


def create_fireworks_math_agent(api_key: Optional[str] = None, **config):
    """Create a math-specialized agent via Fireworks"""
    return create_fireworks_agent(
        model_name="accounts/fireworks/models/qwen2p5-72b-instruct",
        api_key=api_key,
        capabilities=[
            "fast_math_solving",
            "mathematical_reasoning",
            "problem_solving",
            "step_by_step_solutions",
            "multilingual_math"
        ],
        system_prompt="""You are Qwen 2.5 running on Fireworks AI, specialized in mathematics and logical reasoning. 
        You excel at solving complex mathematical problems with clear, step-by-step explanations.""",
        **config
    ) 