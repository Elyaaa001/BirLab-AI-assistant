import asyncio
import json
from typing import Dict, Any, Optional
import aiohttp
from .base_connector import AIModelConnector


class GroqConnector(AIModelConnector):
    """
    Connector for Groq's super-fast inference API.
    
    Groq provides extremely fast inference for various open-source models
    using their custom LPU (Language Processing Unit) chips.
    """
    
    def __init__(self, model_name: str = "llama-3.1-70b-versatile", api_key: Optional[str] = None,
                 base_url: str = "https://api.groq.com/openai/v1", **config):
        super().__init__(model_name, config)
        self.api_key = api_key or config.get("api_key")
        self.base_url = base_url.rstrip('/')
        
        if not self.api_key:
            raise ValueError("Groq API key is required")
    
    async def generate_response(self, prompt: str, **kwargs) -> str:
        """
        Generate a response using Groq's ultra-fast inference API.
        
        Args:
            prompt: The input prompt
            **kwargs: Additional parameters
            
        Returns:
            Generated response string (blazingly fast!)
        """
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        # Prepare messages for chat format
        messages = [{"role": "user", "content": prompt}]
        
        # Add system message if provided
        if "system_prompt" in kwargs:
            messages.insert(0, {"role": "system", "content": kwargs["system_prompt"]})
        
        payload = {
            "model": self.model_name,
            "messages": messages,
            "temperature": kwargs.get("temperature", 0.7),
            "max_tokens": kwargs.get("max_tokens", 1000),
            "top_p": kwargs.get("top_p", 1.0),
            "stream": False,
            "frequency_penalty": kwargs.get("frequency_penalty", 0),
            "presence_penalty": kwargs.get("presence_penalty", 0)
        }
        
        # Add stop sequences if provided
        if "stop" in kwargs:
            payload["stop"] = kwargs["stop"] if isinstance(kwargs["stop"], list) else [kwargs["stop"]]
        
        async with aiohttp.ClientSession() as session:
            try:
                async with session.post(
                    f"{self.base_url}/chat/completions",
                    headers=headers,
                    json=payload,
                    timeout=aiohttp.ClientTimeout(total=30)  # Short timeout for fast inference
                ) as response:
                    
                    if response.status != 200:
                        error_text = await response.text()
                        raise Exception(f"Groq API error {response.status}: {error_text}")
                    
                    result = await response.json()
                    
                    if "choices" not in result or not result["choices"]:
                        raise Exception("No response generated by Groq")
                    
                    return result["choices"][0]["message"]["content"]
                    
            except aiohttp.ClientError as e:
                raise Exception(f"Network error communicating with Groq: {str(e)}")
            except json.JSONDecodeError as e:
                raise Exception(f"Failed to parse Groq response: {str(e)}")
    
    async def stream_response(self, prompt: str, **kwargs):
        """
        Generate a streaming response from Groq (extremely fast tokens!).
        
        Args:
            prompt: The input prompt
            **kwargs: Additional parameters
            
        Yields:
            Response chunks as they arrive at blazing speed
        """
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        messages = [{"role": "user", "content": prompt}]
        
        if "system_prompt" in kwargs:
            messages.insert(0, {"role": "system", "content": kwargs["system_prompt"]})
        
        payload = {
            "model": self.model_name,
            "messages": messages,
            "temperature": kwargs.get("temperature", 0.7),
            "max_tokens": kwargs.get("max_tokens", 1000),
            "top_p": kwargs.get("top_p", 1.0),
            "stream": True
        }
        
        async with aiohttp.ClientSession() as session:
            try:
                async with session.post(
                    f"{self.base_url}/chat/completions",
                    headers=headers,
                    json=payload,
                    timeout=aiohttp.ClientTimeout(total=60)
                ) as response:
                    
                    if response.status != 200:
                        error_text = await response.text()
                        raise Exception(f"Groq streaming error {response.status}: {error_text}")
                    
                    async for line in response.content:
                        line = line.decode('utf-8').strip()
                        if line.startswith('data: '):
                            data_str = line[6:]
                            if data_str == '[DONE]':
                                break
                            try:
                                data = json.loads(data_str)
                                if "choices" in data and data["choices"]:
                                    delta = data["choices"][0].get("delta", {})
                                    if "content" in delta:
                                        yield delta["content"]
                            except json.JSONDecodeError:
                                continue
                    
            except aiohttp.ClientError as e:
                raise Exception(f"Network error during streaming: {str(e)}")
    
    async def validate_connection(self) -> bool:
        """Validate connection to Groq API"""
        try:
            response = await self.generate_response(
                "Hi",
                max_tokens=5,
                temperature=0
            )
            return len(response.strip()) > 0
        except Exception as e:
            self.logger.error(f"Connection validation failed: {e}")
            return False
    
    def get_model_info(self) -> Dict[str, Any]:
        """Get information about the Groq model"""
        model_info = {
            "provider": "Groq",
            "model_name": self.model_name,
            "base_url": self.base_url,
            "hardware": "LPU (Language Processing Unit)",
            "special_feature": "Ultra-fast inference speed"
        }
        
        # Add model-specific information
        if "llama-3" in self.model_name.lower():
            model_info.update({
                "model_family": "Llama 3",
                "architecture": "Transformer (optimized for LPU)",
                "capabilities": [
                    "text_generation",
                    "conversation", 
                    "reasoning",
                    "ultra_fast_inference",
                    "code_generation",
                    "analysis"
                ]
            })
            
            if "70b" in self.model_name.lower():
                model_info.update({
                    "size": "70B parameters",
                    "context_length": 131072,
                    "performance": "Highest quality, blazing fast"
                })
            elif "8b" in self.model_name.lower():
                model_info.update({
                    "size": "8B parameters", 
                    "context_length": 131072,
                    "performance": "Extremely fast, good quality"
                })
                
        elif "mixtral" in self.model_name.lower():
            model_info.update({
                "model_family": "Mixtral",
                "architecture": "Mixture of Experts",
                "capabilities": [
                    "text_generation",
                    "multilingual",
                    "reasoning",
                    "ultra_fast_inference",
                    "complex_tasks"
                ],
                "size": "8x7B parameters",
                "context_length": 32768,
                "performance": "Fast inference with MoE efficiency"
            })
            
        elif "gemma" in self.model_name.lower():
            model_info.update({
                "model_family": "Gemma",
                "architecture": "Transformer (Google)",
                "capabilities": [
                    "text_generation",
                    "conversation",
                    "ultra_fast_inference",
                    "efficient_processing"
                ]
            })
            
            if "7b" in self.model_name.lower():
                model_info.update({
                    "size": "7B parameters",
                    "performance": "Fast and efficient"
                })
        
        # Add speed information
        model_info["speed_features"] = [
            "Sub-second response times",
            "High tokens per second",
            "Optimized for LPU hardware",
            "Minimal latency"
        ]
        
        return model_info
    
    async def get_available_models(self) -> Dict[str, Any]:
        """Get list of available models from Groq"""
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(
                    f"{self.base_url}/models",
                    headers=headers,
                    timeout=aiohttp.ClientTimeout(total=30)
                ) as response:
                    
                    if response.status != 200:
                        return {"error": f"API error {response.status}"}
                    
                    result = await response.json()
                    
                    models = []
                    for model in result.get("data", []):
                        models.append({
                            "id": model.get("id"),
                            "object": model.get("object"),
                            "created": model.get("created"),
                            "owned_by": model.get("owned_by", "Groq"),
                            "context_length": model.get("context_window", "Unknown")
                        })
                    
                    return {
                        "available_models": models,
                        "total_count": len(models),
                        "note": "All models optimized for ultra-fast inference on LPU hardware"
                    }
                    
        except Exception as e:
            self.logger.error(f"Failed to get available models: {e}")
            return {"error": str(e)}


# Convenience functions
def create_groq_agent(model_name: str = "llama-3.1-70b-versatile",
                     api_key: Optional[str] = None,
                     capabilities: Optional[list] = None,
                     system_prompt: str = "",
                     **config) -> 'AIAgent':
    """Create an AI agent using Groq's super-fast inference"""
    from .base_connector import AIAgent
    
    if capabilities is None:
        if "llama-3" in model_name.lower():
            capabilities = [
                "ultra_fast_responses",
                "conversation",
                "reasoning",
                "text_generation",
                "code_generation",
                "analysis"
            ]
        elif "mixtral" in model_name.lower():
            capabilities = [
                "ultra_fast_responses",
                "multilingual",
                "reasoning",
                "complex_tasks",
                "mixture_of_experts"
            ]
        elif "gemma" in model_name.lower():
            capabilities = [
                "ultra_fast_responses",
                "conversation",
                "efficient_processing",
                "text_generation"
            ]
        else:
            capabilities = [
                "ultra_fast_responses",
                "text_generation",
                "conversation"
            ]
    
    connector = GroqConnector(model_name, api_key, **config)
    agent = AIAgent(connector, capabilities, name=f"Groq_{model_name.split('-')[0]}")
    
    if not system_prompt:
        system_prompt = f"""You are powered by Groq's ultra-fast LPU inference running {model_name}. 
        You provide lightning-fast responses while maintaining high quality. Your speed is your superpower - 
        use it to provide quick, helpful, and accurate answers."""
    
    agent.set_system_prompt(system_prompt)
    return agent


def create_groq_speed_demon(api_key: Optional[str] = None, **config):
    """Create the fastest possible Groq agent for rapid responses"""
    return create_groq_agent(
        model_name="llama-3.1-8b-instant", 
        api_key=api_key,
        capabilities=[
            "lightning_fast_responses",
            "instant_answers",
            "rapid_conversation",
            "speed_optimized"
        ],
        system_prompt="""You are the Speed Demon - the fastest AI agent in the system! 
        Powered by Groq's LPU technology, you provide instant, rapid-fire responses. 
        Keep answers concise but helpful. Speed is your game!""",
        **config
    )


def create_groq_powerhouse(api_key: Optional[str] = None, **config):
    """Create a powerful Groq agent with the largest model for complex tasks"""
    return create_groq_agent(
        model_name="llama-3.1-70b-versatile",
        api_key=api_key,
        capabilities=[
            "complex_reasoning",
            "detailed_analysis", 
            "fast_large_model",
            "versatile_tasks",
            "comprehensive_responses"
        ],
        system_prompt="""You are the Groq Powerhouse - combining the capability of a 70B parameter model 
        with blazing-fast inference speed. Handle complex tasks, detailed analysis, and sophisticated 
        reasoning while maintaining incredible speed.""",
        **config
    )


def create_groq_multilingual(api_key: Optional[str] = None, **config):
    """Create a multilingual Groq agent using Mixtral"""
    return create_groq_agent(
        model_name="mixtral-8x7b-32768",
        api_key=api_key,
        capabilities=[
            "multilingual_processing",
            "fast_translation",
            "cross_language_reasoning",
            "mixture_of_experts",
            "cultural_awareness"
        ],
        system_prompt="""You are the Groq Multilingual Master powered by Mixtral's mixture-of-experts architecture. 
        You excel at multilingual tasks, translations, and cross-cultural communication - all at incredible speed 
        thanks to Groq's LPU technology.""",
        **config
    ) 